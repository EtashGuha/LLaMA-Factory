### model
model_name_or_path: /import/ml-sc-nlpcheckpoints-scratch3/jonathanl/generic_checkpoints/llama3.2/Llama-3.2-11B-Vision-Instruct  # Path to your custom checkpoint

### method
stage: sft  # Supervised Fine-Tuning
do_train: true
finetuning_type: full  # Full fine-tuning (adjust if using LoRA or other methods)
deepspeed: /import/ml-sc-scratch5/viekashv/llama3_2_finetuning/LLaMA-Factory/examples/deepspeed/jonathan.json

### dataset
dataset: adi-finetune-train  # Your training dataset from dataset_info.json
template: llama3_vl  # Template format (can be changed based on your use case)
cutoff_len: 8192  # Maximum sequence length
overwrite_cache: true
preprocessing_num_workers: 4  # Adjust based on your system capacity

### eval
eval_dataset: adi-finetune-val  # Your evaluation dataset from dataset_info.json
per_device_eval_batch_size: 4  # Adjust based on GPU memory
eval_strategy: steps
eval_steps: 100  # Evaluate every 100 steps

### output
output_dir: /import/ml-sc-scratch5/viekashv/llama_finetune_output  # Directory to save the fine-tuned model
logging_steps: 1  # Log every 10 steps
save_steps: 5000  # Save model every 5000 steps
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 4  # Adjust based on your GPU memory
gradient_accumulation_steps: 8  # Adjust based on memory and batch size
learning_rate: 2.0e-5  # Fine-tuning learning rate
num_train_epochs: 3  # Number of epochs to train
lr_scheduler_type: cosine  # Learning rate schedule (e.g., cosine, linear)
warmup_ratio: 0.03  # Warmup ratio for learning rate
weight_decay: 0.0  # Weight decay
bf16: true  # Enable bfloat16 (set to true if supported by your hardware)
ddp_timeout: 180000000  # Timeout for distributed training

### report 
report_to: wandb  # Reporting platform (WandB)
run_name: "llama_finetune_adi_data"  # Custom run name for tracking

